{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SIOZocxBjbJ",
        "colab_type": "text"
      },
      "source": [
        "## GoogleDriveをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT6wLMH7CJWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDbgPus4DDIc",
        "colab_type": "text"
      },
      "source": [
        "### パスを通す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQoRAmKtCo6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "PWD = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "sys.path.append(PWD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1kQbcXvD1vj",
        "colab_type": "text"
      },
      "source": [
        "## プログラム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BDxkBEtCoyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "from models.simple_net import SimpleConvNet\n",
        "from models.simple_net import SimpleOctConvNet\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY_mpXHCD7Yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter:\n",
        "    # 平均値測定器\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        assert self.count > 0\n",
        "        return self.sum / self.count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKWQs2RLD-Kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_accuracy(output_data, target_data):\n",
        "    batch_size = target_data.size(0)\n",
        "    _, pred_idx = output_data.topk(1, 1, True, True)  # 出力が最大のインデックスを取り出す\n",
        "    pred_idx = pred_idx.t().squeeze().cpu()\n",
        "    correct = pred_idx.eq(target_data)  # 正解: 1, 不正解: 0\n",
        "    return (correct.sum().float() / batch_size) * 100.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Oo0ZasEC3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, data_loader, model, loss_func, optimizer, epoch, writer, log_step, device):\n",
        "    # 学習モード\n",
        "    model.train()\n",
        "\n",
        "    loss_avg = AverageMeter()\n",
        "    acc_avg = AverageMeter()\n",
        "    batch_time_avg = AverageMeter()\n",
        "\n",
        "    for i, (input_data, target_data) in enumerate(data_loader):\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        output_data = model(input_data.to(device))\n",
        "        loss = loss_func(output_data, target_data.to(device))\n",
        "        loss_avg.update(loss.item(), target_data.size(0))\n",
        "\n",
        "        acc = calc_accuracy(output_data.data, target_data)\n",
        "        acc_avg.update(acc.item(), target_data.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time = time.perf_counter() - start_time\n",
        "        batch_time_avg.update(batch_time)\n",
        "\n",
        "        if ((i + 1) % args.log_interval) == 0:\n",
        "            print(\n",
        "                \"Train: Epoch[{:03}/{:03}], Step[{:04}/{:04}], Loss[{:.4f}(Avg:{:.4f})], Acc[{:.3f}%(Avg:{:.3f}%)], Time[{:.3f}s(Avg:{:.3f}s)]\".format(\n",
        "                    epoch + 1,\n",
        "                    args.max_epoch,\n",
        "                    i + 1,\n",
        "                    len(data_loader),\n",
        "                    loss.item(),\n",
        "                    loss_avg.avg,\n",
        "                    acc.item(),\n",
        "                    acc_avg.avg,\n",
        "                    batch_time * args.log_interval,\n",
        "                    batch_time_avg.avg * args.log_interval,\n",
        "                )\n",
        "            )\n",
        "            if writer:\n",
        "                writer.add_scalar(\"train/loss\", loss.item(), log_step)\n",
        "                writer.add_scalar(\"train/acc\", acc.item(), log_step)\n",
        "                log_step += 1\n",
        "    print(\"Acc(Train)[{:.3f}%], Time[{:.3f}s]\".format(acc_avg.avg, batch_time_avg.sum))\n",
        "    return log_step\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX4QWGIoEIgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(args, data_loader, model, loss_func, epoch, writer, log_step, device):\n",
        "    # 評価モード\n",
        "    model.eval()\n",
        "\n",
        "    loss_avg = AverageMeter()\n",
        "    acc_avg = AverageMeter()\n",
        "    batch_time_avg = AverageMeter()\n",
        "\n",
        "    for i, (input_data, target_data) in enumerate(data_loader):\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        output_data = model(input_data.to(device))\n",
        "        loss = loss_func(output_data, target_data.to(device))\n",
        "        loss_avg.update(loss.item(), target_data.size(0))\n",
        "\n",
        "        acc = calc_accuracy(output_data.data, target_data)\n",
        "        acc_avg.update(acc.item(), target_data.size(0))\n",
        "\n",
        "        batch_time = time.perf_counter() - start_time\n",
        "        batch_time_avg.update(batch_time)\n",
        "\n",
        "        if ((i + 1) % args.log_interval) == 0:\n",
        "            print(\n",
        "                \"Validate: Epoch[{:03}/{:03}], Step[{:04}/{:04}], Loss[{:.4f}(Avg:{:.4f})], Acc[{:.3f}%(Avg:{:.3f}%)], Time[{:.3f}s(Avg:{:.3f}s)]\".format(\n",
        "                    epoch + 1,\n",
        "                    args.max_epoch,\n",
        "                    i + 1,\n",
        "                    len(data_loader),\n",
        "                    loss.item(),\n",
        "                    loss_avg.avg,\n",
        "                    acc.item(),\n",
        "                    acc_avg.avg,\n",
        "                    batch_time * args.log_interval,\n",
        "                    batch_time_avg.avg * args.log_interval,\n",
        "                )\n",
        "            )\n",
        "            if writer:\n",
        "                writer.add_scalar(\"validate/loss\", loss.item(), log_step)\n",
        "                writer.add_scalar(\"validate/acc\", acc.item(), log_step)\n",
        "                log_step += 1\n",
        "    print(\"Acc(Validate)[{:.3f}%], Time[{:.3f}s]\".format(acc_avg.avg, batch_time_avg.sum))\n",
        "    return acc_avg.avg, log_step\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fSyyjWVELiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--datasets_dir\", type=str, default=PWD+\"./datasets\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--innter_channnels\", type=int, default=64)\n",
        "    parser.add_argument(\"--max_epoch\", type=int, default=50)\n",
        "    parser.add_argument(\"--log_interval\", type=int, default=100)\n",
        "    parser.add_argument(\"--use_logfile\", type=bool, default=True)\n",
        "    parser.add_argument(\"--use_octconv\", type=bool, default=True)\n",
        "    parser.add_argument(\"--use_gpu\", type=bool, default=True)\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    if args.use_logfile:\n",
        "        if args.use_octconv:\n",
        "            writer = SummaryWriter(PWD+\"./logs/cifar10-octconv\")\n",
        "        else:\n",
        "            writer = SummaryWriter(PWD+\"./logs/cifar10\")\n",
        "    else:\n",
        "        writer = None\n",
        "\n",
        "    # サポート対象のGPUがあれば使う\n",
        "    if args.use_gpu:\n",
        "        print(\"Check GPU available\")\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    if device == \"cuda\":\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "    print(\"Use devide: {}\".format(device))\n",
        "\n",
        "    # 前処理(学習用)\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: F.pad(x.unsqueeze(0), (4, 4, 4, 4), mode='reflect').squeeze()),  # パディング\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomCrop(32),              # ランダムでピクセル欠け\n",
        "        transforms.RandomHorizontalFlip(),      # ランダムで上下反転\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,)),\n",
        "    ])\n",
        "    # 前処理(検証用)\n",
        "    transform_validate = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    # データセットの取得\n",
        "    train_data_set = datasets.CIFAR10(args.datasets_dir, train=True, transform=transform_train, download=True)\n",
        "    validate_data_set = datasets.CIFAR10(args.datasets_dir, train=False, transform=transform_validate, download=True)\n",
        "\n",
        "    # データローダーの割り当て\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_data_set, batch_size=args.batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
        "    )\n",
        "    validate_data_loader = torch.utils.data.DataLoader(\n",
        "        validate_data_set, batch_size=args.batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # モデル定義\n",
        "    if args.use_octconv:\n",
        "        model = SimpleOctConvNet(args.innter_channnels)\n",
        "    else:\n",
        "        model = SimpleConvNet(args.innter_channnels)\n",
        "    print(model)\n",
        "    model.to(device)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    best_acc = 0\n",
        "    train_log_step = 0\n",
        "    validate_log_step = 0\n",
        "    for epoch in range(args.max_epoch):\n",
        "        train_log_step = train(args, train_data_loader, model, loss_func, optimizer, epoch, writer, train_log_step, device)\n",
        "        epoch_acc, validate_log_step = validate(args, validate_data_loader, model, loss_func, epoch, writer, validate_log_step, device)\n",
        "        best_acc = max(epoch_acc, best_acc)\n",
        "\n",
        "    print(\"Best accuracy[{:.3f}%]\".format(best_acc))\n",
        "    if writer:\n",
        "        writer.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxk8VkjjERpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}